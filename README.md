# ğŸ“Š Stock Data Pipeline

This project showcases a modern data engineering pipeline that automates the daily ingestion, transformation, validation, and reporting of stock price data. It is designed to demonstrate key data engineering concepts such as orchestration, ELT design, data quality checks, and analytics workflow automation.

---

## ğŸš€ Key Features

- **Daily Scheduling with Dagster**  
  Automatically fetches and uploads stock data to BigQuery every morning at 8am EST.

- **Data Ingestion & Storage**  
  Uses `yfinance` to retrieve historical stock data for multiple tickers. Data is stored in a BigQuery table (`raw_stock_prices`).

- **Data Validation**  
  Runs checks to ensure data freshness and completeness before downstream steps.

- **Transformation with dbt**  
  Models include:
  - `average_price_by_day`
  - `volatility_7d`
  - `weekly_aggregates`
  - `final_summary_table`

- **Automated Email Reporting**  
  Generates and sends an Excel summary with key stats (date range, row counts, averages) to a designated recipient via Gmail API.

---

## ğŸ› ï¸ Tech Stack

- **Orchestration**: Dagster
- **Data Ingestion**: Python, yfinance
- **Cloud Storage**: Google BigQuery
- **Transformations**: dbt (BigQuery adapter)
- **Validation & Reporting**: Pandas, openpyxl
- **Notification**: Gmail API (token auth)

---

## ğŸ“ Project Structure

```
stock_project/
â”‚
â”œâ”€â”€ stock_pipeline/              # Dagster logic
â”‚   â””â”€â”€ resources/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ dbt.py               # dbt resource config
â”‚       â”œâ”€â”€ dbt_ops.py           # runs dbt models via Dagster
â”‚       â”œâ”€â”€ fetch_job.py         # orchestrates the full pipeline
â”‚       â”œâ”€â”€ schedules.py         # schedule config for Dagster
â”‚       â”œâ”€â”€ email_utils.py       # Gmail API email logic
â”‚       â”œâ”€â”€ summary_utils.py     # Excel summary generation
â”‚       â””â”€â”€ validate.py          # basic data quality check
â”‚
â”œâ”€â”€ stock_dbt/                   # dbt transformation logic
â”‚   â”œâ”€â”€ models/                  # core dbt models (SQL)
â”‚   â”œâ”€â”€ macros/                  # reusable SQL macros (optional)
â”‚   â”œâ”€â”€ seeds/                   # CSV seed files (if any)
â”‚   â”œâ”€â”€ snapshots/               # dbt snapshots (if used)
â”‚   â”œâ”€â”€ tests/                   # dbt tests
â”‚   â”œâ”€â”€ dbt_project.yml          # dbt project config
â”‚   â”œâ”€â”€ .gitignore
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .env                         # Local environment variables
â”œâ”€â”€ pyproject.toml               # Dagster entrypoint
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project description (this file)
```

---

## ğŸ“Œ How to Run Locally

1. Create and activate a virtual environment
2. Install dependencies  
   `pip install -r requirements.txt`
3. Set up `.env` with BigQuery & Gmail credentials
4. Run job locally:  
   `dagster job execute -m stock_pipeline -j fetch_job`
5. (Optional) Launch Dagster UI:  
   `dagster dev`

---

## ğŸ“¬ Example Output

- Stock data uploaded to BigQuery: `stock_data.raw_stock_prices`
- Email received with Excel summary:
  - Sheet 1: Standard Summary
  - Sheet 2: dbt Final Model Summary



